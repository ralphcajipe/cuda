{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE: This is a reader‑optimized version of the original NVIDIA \"An Even Easier Introduction to CUDA\" notebook.\n",
    ">\n",
    "> Modifications (by Ralph Cajipe):\n",
    "> - Condensed verbose paragraphs into concise, skimmable explanations.\n",
    "> - Preserved all original external links and code semantics.\n",
    "> - Standardized terminology (host/device, kernel, grid/block, grid‑stride loop).\n",
    "> - Highlighted progressive learning steps (CPU -> single thread -> block -> multi‑block grid).\n",
    "> - Removed redundancy; grouped related concepts logically.\n",
    "> - Kept code cells functionally identical (aside from formatting neutrality).\n",
    "> - Streamlined exercises and resource lists without losing intent.\n",
    ">\n",
    "> Goal: Faster comprehension while retaining technical accuracy and reference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2WkOA4mcN7Hj"
   },
   "source": [
    "# An Even Easier Introduction to CUDA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vuOcUi0fvogW"
   },
   "source": [
    "This notebook accompanies Mark Harris's blog post [_An Even Easier Introduction to CUDA_](https://developer.nvidia.com/blog/even-easier-introduction-cuda/).\n",
    "\n",
    "Want more? See the [NVIDIA DLI](https://nvidia.com/dli) courses:\n",
    "- C/C++: [_Fundamentals of Accelerated Computing with CUDA C/C++_](https://courses.nvidia.com/courses/course-v1:DLI+C-AC-01+V1/about)\n",
    "- Python: [_Fundamentals of Accelerated Computing with CUDA Python_](https://courses.nvidia.com/courses/course-v1:DLI+C-AC-02+V1/about)\n",
    "\n",
    "These offer GPU lab access, Nsight Systems profiling, many exercises, and a certificate. For intermediate/advanced material, browse the DLI [_Accelerated Computing_ catalog](https://www.nvidia.com/en-us/training/online/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V1C6GK_MO5er"
   },
   "source": [
    "<img src=\"https://developer.download.nvidia.com/training/courses/T-AC-01-V1/CUDA_Cube_1K.jpeg\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IcmbR8lZPLRv"
   },
   "source": [
    "This is a minimal intro to CUDA, NVIDIA's parallel computing platform. CUDA C++ lets you launch thousands of lightweight GPU threads for data-parallel work (core to modern AI & [Deep Learning](https://developer.nvidia.com/deep-learning)).\n",
    "\n",
    "Prereqs: C/C++ basics plus a CUDA‑capable NVIDIA GPU (any recent GPU or a cloud instance) and the [CUDA Toolkit](https://developer.nvidia.com/cuda-toolkit) installed.\n",
    "\n",
    "Goal: Start from a plain CPU array add, then progressively parallelize it on the GPU while learning:\n",
    "1. Kernels (`__global__`) & launches `<<< >>>`\n",
    "2. Unified Memory (`cudaMallocManaged`)\n",
    "3. Thread indexing (`threadIdx`, `blockIdx`)\n",
    "4. Blocks vs grid scaling\n",
    "5. Grid-stride loops for flexibility\n",
    "\n",
    "Let's begin.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vDQ9ycz0Qfyf"
   },
   "source": [
    "<img src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2017/01/cuda_ai_cube-625x625.jpg\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wH9Rfms_QtXF"
   },
   "source": [
    "## Starting Simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n5-iUihBQvQt"
   },
   "source": [
    "We'll begin with a CPU C++ program that adds two float arrays of 1M elements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nc-gBqLDQ7AC",
    "outputId": "2502ad34-ff06-4d0f-ac45-8db9c21f72fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing add.cpp\n"
     ]
    }
   ],
   "source": [
    "%%writefile add.cpp\n",
    "\n",
    "#include <iostream>\n",
    "#include <math.h>\n",
    "\n",
    "// function to add the elements of two arrays\n",
    "void add(int n, float *x, float *y)\n",
    "{\n",
    "  for (int i = 0; i < n; i++)\n",
    "      y[i] = x[i] + y[i];\n",
    "}\n",
    "\n",
    "int main(void)\n",
    "{\n",
    "  int N = 1<<20; // 1M elements\n",
    "\n",
    "  float *x = new float[N];\n",
    "  float *y = new float[N];\n",
    "\n",
    "  // initialize x and y arrays on the host\n",
    "  for (int i = 0; i < N; i++) {\n",
    "    x[i] = 1.0f;\n",
    "    y[i] = 2.0f;\n",
    "  }\n",
    "\n",
    "  // Run kernel on 1M elements on the CPU\n",
    "  add(N, x, y);\n",
    "\n",
    "  // Check for errors (all values should be 3.0f)\n",
    "  float maxError = 0.0f;\n",
    "  for (int i = 0; i < N; i++)\n",
    "    maxError = fmax(maxError, fabs(y[i]-3.0f));\n",
    "  std::cout << \"Max error: \" << maxError << std::endl;\n",
    "\n",
    "  // Free memory\n",
    "  delete [] x;\n",
    "  delete [] y;\n",
    "\n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gw6DsX4uRHMg"
   },
   "source": [
    "Running the previous cell writes `add.cpp`.\n",
    "Next: compile and then run it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gNpH54M_RbAU",
    "outputId": "6f44f28b-8090-4711-d756-bacc626e88cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%shell\n",
    "g++ add.cpp -o add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I6V2tGPYRi3l"
   },
   "source": [
    "Then run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QmA4ACe5RuiU",
    "outputId": "3f69f0c6-d362-4963-99a2-45f33443e051"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max error: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%shell\n",
    "./add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9IAWYlniR153"
   },
   "source": [
    "Output shows correct result (max error 0). To move work to the GPU turn `add` into a CUDA kernel by adding `__global__`. This marks it as device code callable from host code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "heY-lpzjSHfB"
   },
   "source": [
    "```cpp\n",
    "// CUDA Kernel function to add the elements of two arrays on the GPU\n",
    "__global__\n",
    "void add(int n, float *x, float *y)\n",
    "{\n",
    "  for (int i = 0; i < n; i++)\n",
    "      y[i] = x[i] + y[i];\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kozMbHdpSKNu"
   },
   "source": [
    "Terminology: `__global__` function = kernel. GPU code = device code. CPU code = host code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VhnBGGU-SWiN"
   },
   "source": [
    "## Memory Allocation in CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RvIDRBk2SbqA"
   },
   "source": [
    "Use [Unified Memory](https://developer.nvidia.com/blog/unified-memory-in-cuda-6/) for simplicity: one pointer usable on CPU and GPU. Replace `new/delete` with `cudaMallocManaged` / `cudaFree`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IxCut_urS46H"
   },
   "source": [
    "```cpp\n",
    "  // Allocate Unified Memory -- accessible from CPU or GPU\n",
    "  float *x, *y;\n",
    "  cudaMallocManaged(&x, N*sizeof(float));\n",
    "  cudaMallocManaged(&y, N*sizeof(float));\n",
    "\n",
    "  ...\n",
    "\n",
    "  // Free memory\n",
    "  cudaFree(x);\n",
    "  cudaFree(y);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2oEf2B-1S-1V"
   },
   "source": [
    "Launch kernels with `<<<gridSize, blockSize>>>`. For now `<<<1,1>>>` runs one thread.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqTJlvWLS7iW"
   },
   "source": [
    "```cpp\n",
    "add<<<1, 1>>>(N, x, y);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RGf0ZiTOTTHU"
   },
   "source": [
    "Kernels launch asynchronously; call `cudaDeviceSynchronize()` before reading results. Full minimal GPU version:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K8bYDM7kYT7S",
    "outputId": "7ade87ba-865f-43a3-91cf-596e22ff0ac4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing add.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile add.cu\n",
    "\n",
    "#include <iostream>\n",
    "#include <math.h>\n",
    "// Kernel function to add the elements of two arrays\n",
    "__global__\n",
    "void add(int n, float *x, float *y)\n",
    "{\n",
    "  for (int i = 0; i < n; i++)\n",
    "    y[i] = x[i] + y[i];\n",
    "}\n",
    "\n",
    "int main(void)\n",
    "{\n",
    "  int N = 1<<20\n",
    " ;\n",
    "  float *x, *y;\n",
    "\n",
    "  // Allocate Unified Memory – accessible from CPU or GPU\n",
    "  cudaMallocManaged(&x, N*sizeof(float));\n",
    "  cudaMallocManaged(&y, N*sizeof(float));\n",
    "\n",
    "  // initialize x and y arrays on the host\n",
    "  for (int i = 0; i < N; i++) {\n",
    "    x[i] = 1.0f;\n",
    "    y[i] = 2.0f;\n",
    "  }\n",
    "\n",
    "  // Run kernel on 1M elements on the GPU\n",
    "  add<<<1, 1>>>(N, x, y);\n",
    "\n",
    "  // Wait for GPU to finish before accessing on host\n",
    "  cudaDeviceSynchronize();\n",
    "\n",
    "  // Check for errors (all values should be 3.0f)\n",
    "  float maxError = 0.0f;\n",
    "  for (int i = 0; i < N; i++)\n",
    "    maxError = fmax(maxError, fabs(y[i]-3.0f));\n",
    "  std::cout << \"Max error: \" << maxError << std::endl;\n",
    "\n",
    "  // Free memory\n",
    "  cudaFree(x);\n",
    "  cudaFree(y);\n",
    "\n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TjLGGp0oYeEc",
    "outputId": "886f1d2e-252b-4a95-857e-20a7095a829d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max error: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%shell\n",
    "\n",
    "nvcc add.cu -o add_cuda\n",
    "./add_cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ATssEzEYqGx"
   },
   "source": [
    "This kernel is only correct for a single thread. Multiple threads would redundantly traverse the whole array and race on each element.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3kKpDoZ-YzJ8"
   },
   "source": [
    "## Profile it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r-BC-CWVZglt"
   },
   "source": [
    "Profile runtime with `nvprof ./add_cuda` (CLI profiler from the CUDA Toolkit).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gtfQLWwYZpfV",
    "outputId": "735a4bb2-0f19-4af0-d1fc-922b6a5a9649"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==1238== NVPROF is profiling process 1238, command: ./add_cuda\n",
      "Max error: 1\n",
      "==1238== Profiling application: ./add_cuda\n",
      "==1238== Profiling result:\n",
      "No kernels were profiled.\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      "      API calls:   96.30%  216.98ms         2  108.49ms  55.053us  216.93ms  cudaMallocManaged\n",
      "                    3.50%  7.8856ms         1  7.8856ms  7.8856ms  7.8856ms  cudaLaunchKernel\n",
      "                    0.13%  292.94us         2  146.47us  117.54us  175.40us  cudaFree\n",
      "                    0.06%  126.09us       114  1.1060us     104ns  51.635us  cuDeviceGetAttribute\n",
      "                    0.01%  12.388us         1  12.388us  12.388us  12.388us  cuDeviceGetName\n",
      "                    0.00%  9.9530us         1  9.9530us  9.9530us  9.9530us  cudaDeviceSynchronize\n",
      "                    0.00%  5.6130us         1  5.6130us  5.6130us  5.6130us  cuDeviceGetPCIBusId\n",
      "                    0.00%  1.4870us         3     495ns     133ns  1.1100us  cuDeviceGetCount\n",
      "                    0.00%     962ns         2     481ns     156ns     806ns  cuDeviceGet\n",
      "                    0.00%     850ns         1     850ns     850ns     850ns  cuModuleGetLoadingMode\n",
      "                    0.00%     412ns         1     412ns     412ns     412ns  cuDeviceTotalMem\n",
      "                    0.00%     256ns         1     256ns     256ns     256ns  cuDeviceGetUuid\n",
      "\n",
      "==1238== Unified Memory profiling result:\n",
      "Total CPU Page faults: 24\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%shell\n",
    "\n",
    "nvprof ./add_cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F9Dn4ZV-Z_UJ"
   },
   "source": [
    "Above output shows the single `add` call. To see which GPU you have (e.g. Tesla T4) run:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TrYmwVZfaPqz",
    "outputId": "a39c9902-8975-4a7a-8b5d-d1155010ddd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Aug 30 16:16:04 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   46C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%shell\n",
    "\n",
    "nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-MWYteAVadCs"
   },
   "source": [
    "Let's make it faster with parallelism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SaiMC73Falvb"
   },
   "source": [
    "## Picking up the Threads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KDFuBr_2apuJ"
   },
   "source": [
    "Execution config `<<<blocks, threadsPerBlock>>>`. Start by increasing threads per block: `<<<1,256>>>` (multiples of 32 are typical).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a2Pmyj0KavgB"
   },
   "source": [
    "```cpp\n",
    "add<<<1, 256>>>(N, x, y);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oAYpH9Ctay5G"
   },
   "source": [
    "To partition work, use built-ins: `threadIdx.x` (thread id in block), `blockDim.x` (threads per block). Stride loop lets each thread handle a slice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TSiqhFK_a6N3"
   },
   "source": [
    "```cpp\n",
    "__global__\n",
    "void add(int n, float *x, float *y)\n",
    "{\n",
    "  int index = threadIdx.x;\n",
    "  int stride = blockDim.x;\n",
    "  for (int i = index; i < n; i += stride)\n",
    "      y[i] = x[i] + y[i];\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_7mYcBzOa9zR"
   },
   "source": [
    "Setting `index=0`, `stride=1` matches original. Save as `add_block.cu`, compile, profile.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "goCKY9QNbPZ-",
    "outputId": "33fd5bf3-345b-4f6b-bd0a-4e80ba77cf66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing add_block.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile add_block.cu\n",
    "\n",
    "#include <iostream>\n",
    "#include <math.h>\n",
    "\n",
    "// Kernel function to add the elements of two arrays\n",
    "__global__\n",
    "void add(int n, float *x, float *y)\n",
    "{\n",
    "  int index = threadIdx.x;\n",
    "  int stride = blockDim.x;\n",
    "  for (int i = index; i < n; i += stride)\n",
    "      y[i] = x[i] + y[i];\n",
    "}\n",
    "\n",
    "int main(void)\n",
    "{\n",
    "  int N = 1<<20;\n",
    "  float *x, *y;\n",
    "\n",
    "  // Allocate Unified Memory – accessible from CPU or GPU\n",
    "  cudaMallocManaged(&x, N*sizeof(float));\n",
    "  cudaMallocManaged(&y, N*sizeof(float));\n",
    "\n",
    "  // initialize x and y arrays on the host\n",
    "  for (int i = 0; i < N; i++) {\n",
    "    x[i] = 1.0f;\n",
    "    y[i] = 2.0f;\n",
    "  }\n",
    "\n",
    "  // Run kernel on 1M elements on the GPU\n",
    "  add<<<1, 256>>>(N, x, y);\n",
    "\n",
    "  // Wait for GPU to finish before accessing on host\n",
    "  cudaDeviceSynchronize();\n",
    "\n",
    "  // Check for errors (all values should be 3.0f)\n",
    "  float maxError = 0.0f;\n",
    "  for (int i = 0; i < N; i++)\n",
    "    maxError = fmax(maxError, fabs(y[i]-3.0f));\n",
    "  std::cout << \"Max error: \" << maxError << std::endl;\n",
    "\n",
    "  // Free memory\n",
    "  cudaFree(x);\n",
    "  cudaFree(y);\n",
    "\n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l9cmfbcVbYgD",
    "outputId": "911bda2b-a6cf-4374-c744-91ee702567c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==1337== NVPROF is profiling process 1337, command: ./add_block\n",
      "Max error: 1\n",
      "==1337== Profiling application: ./add_block\n",
      "==1337== Profiling result:\n",
      "No kernels were profiled.\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      "      API calls:   94.81%  227.23ms         2  113.62ms  62.254us  227.17ms  cudaMallocManaged\n",
      "                    4.90%  11.755ms         1  11.755ms  11.755ms  11.755ms  cudaLaunchKernel\n",
      "                    0.19%  459.23us         2  229.61us  219.73us  239.50us  cudaFree\n",
      "                    0.07%  175.84us       114  1.5420us     143ns  70.650us  cuDeviceGetAttribute\n",
      "                    0.01%  13.778us         1  13.778us  13.778us  13.778us  cuDeviceGetName\n",
      "                    0.01%  13.019us         1  13.019us  13.019us  13.019us  cudaDeviceSynchronize\n",
      "                    0.00%  7.5550us         1  7.5550us  7.5550us  7.5550us  cuDeviceGetPCIBusId\n",
      "                    0.00%  2.4270us         3     809ns     188ns  1.9190us  cuDeviceGetCount\n",
      "                    0.00%  1.0260us         1  1.0260us  1.0260us  1.0260us  cuDeviceTotalMem\n",
      "                    0.00%  1.0060us         2     503ns     180ns     826ns  cuDeviceGet\n",
      "                    0.00%     570ns         1     570ns     570ns     570ns  cuModuleGetLoadingMode\n",
      "                    0.00%     287ns         1     287ns     287ns     287ns  cuDeviceGetUuid\n",
      "\n",
      "==1337== Unified Memory profiling result:\n",
      "Total CPU Page faults: 24\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%shell\n",
    "\n",
    "nvcc add_block.cu -o add_block\n",
    "nvprof ./add_block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fo5KaV3Nba7g"
   },
   "source": [
    "Expect ~256x less kernel time vs 1 thread (hardware + overheads may vary). Let's scale further with multiple blocks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YtgQWOyMcPfn"
   },
   "source": [
    "## Out of the Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wAoFGwmbcRbN"
   },
   "source": [
    "GPUs have many SMs running multiple blocks concurrently. Use multiple blocks to use more SMs. Compute blocks: `numBlocks = (N + blockSize - 1)/blockSize`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AnI2II2ockgC"
   },
   "source": [
    "```cpp\n",
    "int blockSize = 256;\n",
    "int numBlocks = (N + blockSize - 1) / blockSize;\n",
    "add<<<numBlocks, blockSize>>>(N, x, y);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ayq2MJZLctY0"
   },
   "source": [
    "<img src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2017/01/cuda_indexing.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fZduP7RWc3Je"
   },
   "source": [
    "Index formula: `globalIndex = blockIdx.x * blockDim.x + threadIdx.x`. Use loop stride = total threads: `blockDim.x * gridDim.x` (grid-stride loop) so any grid size covers N.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6cI2WLEAeG5y"
   },
   "source": [
    "```cpp\n",
    "__global__\n",
    "void add(int n, float *x, float *y)\n",
    "{\n",
    "  int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "  int stride = blockDim.x * gridDim.x;\n",
    "  for (int i = index; i < n; i += stride)\n",
    "    y[i] = x[i] + y[i];\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "83hC-rCLdPHC"
   },
   "source": [
    "Stride = total threads (`blockDim.x * gridDim.x`). This grid-stride loop pattern scales automatically. Save as `add_grid.cu`, compile, profile.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a7w-DHBRdhUC",
    "outputId": "21233355-fd94-45ef-b86f-537de64b240c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing add_grid.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile add_grid.cu\n",
    "\n",
    "#include <iostream>\n",
    "#include <math.h>\n",
    "\n",
    "// Kernel function to add the elements of two arrays\n",
    "__global__\n",
    "void add(int n, float *x, float *y)\n",
    "{\n",
    "  int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "  int stride = blockDim.x * gridDim.x;\n",
    "  for (int i = index; i < n; i += stride)\n",
    "    y[i] = x[i] + y[i];\n",
    "}\n",
    "\n",
    "int main(void)\n",
    "{\n",
    "  int N = 1<<20;\n",
    "  float *x, *y;\n",
    "\n",
    "  // Allocate Unified Memory – accessible from CPU or GPU\n",
    "  cudaMallocManaged(&x, N*sizeof(float));\n",
    "  cudaMallocManaged(&y, N*sizeof(float));\n",
    "\n",
    "  // initialize x and y arrays on the host\n",
    "  for (int i = 0; i < N; i++) {\n",
    "    x[i] = 1.0f;\n",
    "    y[i] = 2.0f;\n",
    "  }\n",
    "\n",
    "  // Run kernel on 1M elements on the GPU\n",
    "  int blockSize = 256;\n",
    "  int numBlocks = (N + blockSize - 1) / blockSize;\n",
    "  add<<<numBlocks, blockSize>>>(N, x, y);\n",
    "\n",
    "  // Wait for GPU to finish before accessing on host\n",
    "  cudaDeviceSynchronize();\n",
    "\n",
    "  // Check for errors (all values should be 3.0f)\n",
    "  float maxError = 0.0f;\n",
    "  for (int i = 0; i < N; i++)\n",
    "    maxError = fmax(maxError, fabs(y[i]-3.0f));\n",
    "  std::cout << \"Max error: \" << maxError << std::endl;\n",
    "\n",
    "  // Free memory\n",
    "  cudaFree(x);\n",
    "  cudaFree(y);\n",
    "\n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FhcrktW9dw34",
    "outputId": "0e8634a5-3544-45b7-f625-68e86f485063"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==1443== NVPROF is profiling process 1443, command: ./add_grid\n",
      "Max error: 1\n",
      "==1443== Profiling application: ./add_grid\n",
      "==1443== Profiling result:\n",
      "No kernels were profiled.\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      "      API calls:   95.98%  205.41ms         2  102.70ms  53.308us  205.35ms  cudaMallocManaged\n",
      "                    3.77%  8.0760ms         1  8.0760ms  8.0760ms  8.0760ms  cudaLaunchKernel\n",
      "                    0.16%  351.93us         2  175.96us  131.63us  220.30us  cudaFree\n",
      "                    0.07%  147.59us       114  1.2940us     112ns  68.757us  cuDeviceGetAttribute\n",
      "                    0.01%  12.679us         1  12.679us  12.679us  12.679us  cuDeviceGetName\n",
      "                    0.01%  11.543us         1  11.543us  11.543us  11.543us  cudaDeviceSynchronize\n",
      "                    0.00%  5.6790us         1  5.6790us  5.6790us  5.6790us  cuDeviceGetPCIBusId\n",
      "                    0.00%  1.4370us         3     479ns     120ns  1.0490us  cuDeviceGetCount\n",
      "                    0.00%  1.1740us         2     587ns     133ns  1.0410us  cuDeviceGet\n",
      "                    0.00%     575ns         1     575ns     575ns     575ns  cuModuleGetLoadingMode\n",
      "                    0.00%     501ns         1     501ns     501ns     501ns  cuDeviceTotalMem\n",
      "                    0.00%     271ns         1     271ns     271ns     271ns  cuDeviceGetUuid\n",
      "\n",
      "==1443== Unified Memory profiling result:\n",
      "Total CPU Page faults: 24\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%shell\n",
    "\n",
    "nvcc add_grid.cu -o add_grid\n",
    "nvprof ./add_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y7Tz-xo3d1oX"
   },
   "source": [
    "Multi-block launch yields another jump (results vary by GPU). If final speedups differ from blog, see Exercise 4.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ja5CiQZpicHC"
   },
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BEijwk25id3t"
   },
   "source": [
    "Try these:\n",
    "1. Browse [CUDA docs](https://docs.nvidia.com/cuda/index.html): install guides, Programming Guide, Best Practices, arch tuning guides.\n",
    "2. Add `printf` in the kernel for `threadIdx.x`, `blockIdx.x`. Observe ordering (not guaranteed sequential).\n",
    "3. Print `threadIdx.y/z`, `blockIdx.y`. Explore 2D/3D launch shapes (set in `<<<grid, block>>>`).\n",
    "4. On a Pascal GPU compare `add_grid.cu` performance. Read about [Pascal UMA + page migration](https://developer.nvidia.com/blog/beyond-gpu-memory-limits-unified-memory-pascal/) and see [Unified Memory for CUDA Beginners](https://developer.nvidia.com/blog/unified-memory-cuda-beginners/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NpWVnIPujp0K"
   },
   "source": [
    "## Where to From Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nTyQePjlkRJ3"
   },
   "source": [
    "Next steps:\n",
    "- C/C++ course: [_Fundamentals of Accelerated Computing with CUDA C/C++_](https://courses.nvidia.com/courses/course-v1:DLI+C-AC-01+V1/about)\n",
    "- Python course: [_Fundamentals of Accelerated Computing with CUDA Python_](https://courses.nvidia.com/courses/course-v1:DLI+C-AC-02+V1/about)\n",
    "- More: DLI [_Accelerated Computing_ catalog](https://www.nvidia.com/en-us/training/online/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick CUDA Recap (What I Learned Here)\n",
    "**Problem we solved:** Adding two large float arrays efficiently. We started with a plain CPU loop and progressively parallelized it on the GPU.\n",
    "\n",
    "**Steps you walked through:**\n",
    "1. CPU baseline: simple for-loop add.\n",
    "2. Turned the function into a CUDA kernel (`__global__`).\n",
    "3. Used Unified Memory (`cudaMallocManaged`) so one pointer works on host & device.\n",
    "4. Launched a kernel with 1 thread (correct but slow) `add<<<1,1>>>`.\n",
    "5. Introduced many threads in one block (`threadIdx.x` + stride) to split the work.\n",
    "6. Scaled to many blocks so multiple SMs run in parallel (`blockIdx.x * blockDim.x + threadIdx.x`).\n",
    "7. Added a grid‑stride loop so any grid size still covers all N elements.\n",
    "8. Profiled runs (`nvprof`) and inspected GPU info (`nvidia-smi`).\n",
    "\n",
    "**Core concepts gained:**\n",
    "- Kernel launch syntax `<<<grid, block>>>`.\n",
    "- Thread & block indexing math.\n",
    "- Grid‑stride loop pattern for scalable kernels.\n",
    "- Unified Memory for fast prototyping (simpler than manual `cudaMemcpy`).\n",
    "- Need for `cudaDeviceSynchronize()` before reading results.\n",
    "- Validating correctness (max error check) after GPU execution.\n",
    "\n",
    "**Mental template you can now reuse:**\n",
    "Prepare data → allocate (Unified or device) → compute global index → loop with stride → write results → synchronize → validate.\n",
    "\n",
    "**Next easy extensions:** Try a different element-wise op (scale, bias, clamp), then a reduction (sum) using shared memory, then fuse multiple operations into one kernel.\n",
    "\n",
    "**One sentence:** You learned how to map a simple array operation from a single CPU loop to thousands of GPU threads using CUDA kernels, indexing, and grid‑stride loops while ensuring correctness and scalability."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
